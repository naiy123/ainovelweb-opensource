/**
 * AI é™é‡/äººæ€§åŒ–æ”¹å†™æ¨¡å—ï¼ˆæœ±é›€é™é‡ï¼‰
 *
 * Pipeline:
 * 1. Gemini Flash è¯­ä¹‰åˆ‡å‰²ï¼ˆ300-400å­—/å—ï¼‰
 * 2. å¾®è°ƒæ¨¡å‹é€å—æ”¹å†™
 * 3. æ‹¼æ¥è¾“å‡º
 */

import { getVertexClient, getModelId, getTemperature, isVertexAIConfigured } from "./providers/vertex/client"

export { isVertexAIConfigured } from "./providers/vertex/client"

// å‚æ•°å’Œç»“æœç±»å‹
export interface HumanizeParams {
  text: string
}

export interface HumanizeResult {
  originalText: string
  humanizedText: string
}

export interface HumanizeStreamChunk {
  type: "content" | "done"
  text?: string
}

// Token ç»Ÿè®¡
interface TokenUsage {
  inputTokens: number
  outputTokens: number
}

interface CostSummary {
  flashInput: number
  flashOutput: number
  rewriteInput: number
  rewriteOutput: number
  totalTokens: number
  totalCost: number  // ç¾å…ƒ
}

// æ¨¡å‹é…ç½®
const FLASH_MODEL = "gemini-2.5-flash"
const REWRITE_SYSTEM_PROMPT = "ä½ æ˜¯ç½‘æ–‡ä½œè€…"

// ä»·æ ¼é…ç½® (ç¾å…ƒ/ç™¾ä¸‡token)
const PRICING = {
  flash: { input: 0.15, output: 0.60 },      // Gemini 2.5 Flash
  rewrite: { input: 0.50, output: 1.50 },    // å¾®è°ƒæ¨¡å‹ (ä¼°ç®—)
}

/**
 * ä»å“åº”ä¸­æå– token ä½¿ç”¨é‡
 */
function extractTokenUsage(response: { usageMetadata?: { promptTokenCount?: number; candidatesTokenCount?: number } }): TokenUsage {
  return {
    inputTokens: response.usageMetadata?.promptTokenCount || 0,
    outputTokens: response.usageMetadata?.candidatesTokenCount || 0,
  }
}

/**
 * Step 1: è¯­ä¹‰åˆ‡å‰² (Gemini Flash)
 * å°†æ–‡æœ¬æŒ‰è¯­ä¹‰å®Œæ•´æ€§åˆ‡å‰²æˆ 300-400 å­—çš„å—
 */
async function semanticSplit(text: string): Promise<{ chunks: string[]; tokens: TokenUsage }> {
  const prompt = `å°†ä»¥ä¸‹æ–‡æœ¬æŒ‰è¯­ä¹‰å®Œæ•´æ€§åˆ‡å‰²æˆå¤šä¸ªæ®µè½å—ã€‚

è¦æ±‚ï¼š
- æ¯å— 500-800 å­—å·¦å³
- åœ¨åœºæ™¯è½¬æ¢ã€æ—¶é—´è·³è·ƒã€å¯¹è¯ç»“æŸç­‰è‡ªç„¶è¾¹ç•Œåˆ‡åˆ†
- ä¿æŒè¯­ä¹‰å®Œæ•´ï¼Œä¸è¦åˆ‡æ–­å¯¹è¯æˆ–åŠ¨ä½œæå†™
- ä¿ç•™åŸæ–‡çš„æ®µè½æ¢è¡Œç¬¦

åŸæ–‡ï¼š
${text}

è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ–‡æœ¬å—ï¼ˆä¿ç•™æ¢è¡Œï¼‰ï¼š
["ç¬¬ä¸€å—...", "ç¬¬äºŒå—...", ...]`

  const client = getVertexClient()
  const model = client.getGenerativeModel({ model: FLASH_MODEL })

  const result = await model.generateContent({
    contents: [{ role: "user", parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: 0.2,
      responseMimeType: "application/json",
    },
  })

  const tokens = extractTokenUsage(result.response)
  const responseText = result.response.candidates?.[0]?.content?.parts?.[0]?.text || "[]"

  let chunks: string[]
  try {
    chunks = JSON.parse(responseText)
  } catch {
    console.error("   âœ— åˆ‡å‰²JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†æ®µ")
    // é™çº§ï¼šæŒ‰æ®µè½ç®€å•åˆ†å‰²
    chunks = simpleSplit(text, 650)
  }

  return { chunks, tokens }
}

/**
 * ç®€å•åˆ†å‰²ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
 */
function simpleSplit(text: string, targetSize: number): string[] {
  const paragraphs = text.split(/\n+/)
  const chunks: string[] = []
  let current = ""

  for (const para of paragraphs) {
    if (current.length + para.length > targetSize && current.length > 0) {
      chunks.push(current.trim())
      current = para
    } else {
      current += (current ? "\n\n" : "") + para
    }
  }

  if (current.trim()) {
    chunks.push(current.trim())
  }

  return chunks
}

/**
 * Step 2: æ”¹å†™å•ä¸ªå— (å¾®è°ƒæ¨¡å‹)
 */
async function rewriteChunk(chunk: string, index: number, total: number): Promise<{ text: string; tokens: TokenUsage }> {
  const prompt = `ä»¥ç½‘æ–‡ä½œå®¶çš„è¡Œæ–‡é£æ ¼é‡å†™ä»¥ä¸‹æ®µè½ã€‚

è¦æ±‚ï¼š
1. é•¿çŸ­å¥äº¤æ›¿ï¼Œæ‰“ç ´åŸæœ‰èŠ‚å¥ï¼ŒèŠ‚å¥å˜å¿«ï¼ˆå¦‚ï¼šé•¿å¥æ‹†æˆ2-3çŸ­å¥ï¼Œæˆ–çŸ­å¥åˆå¹¶ï¼‰
2. é€‚å½“åŠ å…¥å£è¯­åŒ–è¡¨è¾¾
3. å˜æ¢å¥å¼ç»“æ„
4. é¿å…è¿ç»­ä½¿ç”¨ç›¸åŒå¥å¼å¼€å¤´
5. ä¿æŒåŸæ„ï¼Œä¿ç•™äººååœ°å

${chunk}`

  const client = getVertexClient()
  const model = client.getGenerativeModel({
    model: getModelId(),
    systemInstruction: REWRITE_SYSTEM_PROMPT,
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
  } as any)

  const result = await model.generateContent({
    contents: [{ role: "user", parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: getTemperature(),
      maxOutputTokens: chunk.length * 3,
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      thinkingConfig: { thinkingBudget: Math.round(chunk.length * 1.5) },
    } as any,
  })

  const tokens = extractTokenUsage(result.response)
  const responseText = result.response.candidates?.[0]?.content?.parts?.[0]?.text || chunk

  console.log(`   â†’ å— ${index + 1}/${total} (${chunk.length}â†’${responseText.length}å­—)`)

  return { text: responseText, tokens }
}

/**
 * è®¡ç®—æˆæœ¬
 */
function calculateCost(
  flashTokens: TokenUsage,
  rewriteTokens: TokenUsage
): CostSummary {
  const flashInputCost = (flashTokens.inputTokens / 1_000_000) * PRICING.flash.input
  const flashOutputCost = (flashTokens.outputTokens / 1_000_000) * PRICING.flash.output
  const rewriteInputCost = (rewriteTokens.inputTokens / 1_000_000) * PRICING.rewrite.input
  const rewriteOutputCost = (rewriteTokens.outputTokens / 1_000_000) * PRICING.rewrite.output

  return {
    flashInput: flashTokens.inputTokens,
    flashOutput: flashTokens.outputTokens,
    rewriteInput: rewriteTokens.inputTokens,
    rewriteOutput: rewriteTokens.outputTokens,
    totalTokens: flashTokens.inputTokens + flashTokens.outputTokens + rewriteTokens.inputTokens + rewriteTokens.outputTokens,
    totalCost: flashInputCost + flashOutputCost + rewriteInputCost + rewriteOutputCost,
  }
}

/**
 * ä¸»å‡½æ•°ï¼šé™AIç‡æ”¹å†™ï¼ˆæµå¼ï¼‰
 */
export async function* humanizeTextStream(
  params: HumanizeParams
): AsyncGenerator<HumanizeStreamChunk, void, unknown> {
  if (!isVertexAIConfigured()) {
    throw new Error("Vertex AI æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨æœ±é›€é™é‡åŠŸèƒ½")
  }

  const { text } = params

  console.log(`\nğŸ¦ æœ±é›€é™é‡ | ${text.length}å­—`)

  // Step 1: è¯­ä¹‰åˆ‡å‰²
  console.log(`\n   [Step 1] è¯­ä¹‰åˆ‡å‰²`)
  console.log(`   [æ¨¡å‹] ${FLASH_MODEL}`)
  console.log(`   [ç›®æ ‡] 500-800å­—/å—`)

  let chunks: string[]
  let flashTokens: TokenUsage = { inputTokens: 0, outputTokens: 0 }

  try {
    const result = await semanticSplit(text)
    chunks = result.chunks
    flashTokens = result.tokens
    console.log(`   â†’ å®Œæˆï¼Œåˆ‡åˆ†ä¸º ${chunks.length} å—`)
    console.log(`   â†’ Token: ${flashTokens.inputTokens} in / ${flashTokens.outputTokens} out`)
  } catch (error) {
    console.error("   âœ— åˆ‡å‰²å¤±è´¥:", error)
    throw new Error("è¯­ä¹‰åˆ‡å‰²å¤±è´¥")
  }

  // Step 2: é€å—æ”¹å†™
  console.log(`\n   [Step 2] é€å—æ”¹å†™`)
  console.log(`   [æ¨¡å‹] ${getModelId()}`)
  console.log(`   [System] ${REWRITE_SYSTEM_PROMPT}`)
  console.log(`   [æ¸©åº¦] ${getTemperature()}`)

  const rewrittenChunks: string[] = []
  let rewriteTokens: TokenUsage = { inputTokens: 0, outputTokens: 0 }

  for (let i = 0; i < chunks.length; i++) {
    try {
      const result = await rewriteChunk(chunks[i], i, chunks.length)
      rewrittenChunks.push(result.text)
      rewriteTokens.inputTokens += result.tokens.inputTokens
      rewriteTokens.outputTokens += result.tokens.outputTokens
    } catch (error) {
      console.error(`   âœ— å— ${i + 1} å¤±è´¥:`, error)
      rewrittenChunks.push(chunks[i])  // å¤±è´¥æ—¶ä¿ç•™åŸæ–‡
    }
  }

  // Step 3: æ‹¼æ¥è¾“å‡º
  const outputText = rewrittenChunks.join("\n\n")

  // ç»Ÿè®¡
  const cost = calculateCost(flashTokens, rewriteTokens)
  const diff = outputText.length - text.length
  const diffStr = diff >= 0 ? `+${diff}` : `${diff}`

  console.log(`\n   âœ“ å®Œæˆ ${rewrittenChunks.length}/${chunks.length} å—`)
  console.log(`   âœ“ å­—æ•° ${text.length}â†’${outputText.length} (${diffStr})`)
  console.log(`\n   ğŸ“Š Token ç»Ÿè®¡`)
  console.log(`   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”`)
  console.log(`   â”‚ æ¨¡å‹        â”‚ Input    â”‚ Output   â”‚`)
  console.log(`   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤`)
  console.log(`   â”‚ Flash       â”‚ ${String(cost.flashInput).padStart(8)} â”‚ ${String(cost.flashOutput).padStart(8)} â”‚`)
  console.log(`   â”‚ å¾®è°ƒæ¨¡å‹    â”‚ ${String(cost.rewriteInput).padStart(8)} â”‚ ${String(cost.rewriteOutput).padStart(8)} â”‚`)
  console.log(`   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤`)
  console.log(`   â”‚ æ€»è®¡        â”‚ ${String(cost.totalTokens).padStart(19)} â”‚`)
  console.log(`   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜`)
  console.log(`   ğŸ’° é¢„ä¼°æˆæœ¬: $${cost.totalCost.toFixed(4)}\n`)

  yield { type: "content", text: outputText }
  yield { type: "done" }
}

/**
 * é™AIç‡æ”¹å†™ï¼ˆéæµå¼ï¼‰
 */
export async function humanizeText(params: HumanizeParams): Promise<HumanizeResult> {
  const chunks: string[] = []
  for await (const chunk of humanizeTextStream(params)) {
    if (chunk.type === "content" && chunk.text) {
      chunks.push(chunk.text)
    }
  }
  return {
    originalText: params.text,
    humanizedText: chunks.join(""),
  }
}
