/**
 * ç«å±±å¼•æ“æ–‡æœ¬ç”Ÿæˆé€‚é…å™¨
 *
 * ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ï¼Œæ”¯æŒ Doubao ç³»åˆ—æ¨¡å‹
 * ç‰¹ç‚¹ï¼š
 * - æ·±åº¦æ€è€ƒæ¨¡å‹ (doubao-seed-*) è‡ªåŠ¨è§¦å‘æ¨ç†ï¼Œä¸æ”¯æŒ temperature/max_tokens
 * - æ€è€ƒå†…å®¹é€šè¿‡ reasoning_content å­—æ®µè¿”å›
 */

import { volcClient } from "./client"
import type { TextProvider } from "../types"
import type {
  TextGenerateParams,
  UnifiedTextResult,
  TextStreamChunk,
} from "../../types/text"
import type { TokenUsage } from "../../types/common"
import { PROVIDER_CAPABILITIES, supportsThinking } from "../../capabilities"
import { logAIRequest, logAIResponse, logStreamComplete, devLog } from "../../logger"

export class VolcTextProvider implements TextProvider {
  readonly name = "volcengine" as const
  readonly capabilities = PROVIDER_CAPABILITIES.volcengine

  /**
   * å°†ç»Ÿä¸€å‚æ•°è½¬æ¢ä¸ºç«å±±åŸç”Ÿå‚æ•°
   *
   * æ³¨æ„ï¼šæ·±åº¦æ€è€ƒæ¨¡å‹ä¸æ”¯æŒ temperature/max_tokens å‚æ•°
   */
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  private translateParams(params: TextGenerateParams, model: string): any {
    const isThinkingModel = supportsThinking("volcengine", model) || model.includes("seed")

    // æ„å»ºæ¶ˆæ¯
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const messages: any[] = []
    if (params.systemPrompt) {
      messages.push({ role: "system", content: params.systemPrompt })
    }
    messages.push({ role: "user", content: params.userPrompt })

    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const options: any = {
      model,
      messages,
      stream: params.stream ?? false,
    }

    // æ·±åº¦æ€è€ƒæ¨¡å‹ä¸æ”¯æŒè¿™äº›å‚æ•°
    if (!isThinkingModel) {
      if (params.maxTokens) {
        options.max_tokens = params.maxTokens
      }
      if (params.temperature !== undefined) {
        options.temperature = params.temperature
      }
    }

    // åŸç”Ÿå‚æ•°è¦†ç›–
    if (params.nativeOptions) {
      Object.assign(options, params.nativeOptions)
    }

    return options
  }

  /**
   * å°†ç«å±±å“åº”è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
   */
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  private translateResponse(completion: any): UnifiedTextResult {
    const message = completion.choices?.[0]?.message

    return {
      content: message?.content || "",
      // ç«å±±æ·±åº¦æ€è€ƒæ¨¡å‹è¿”å› reasoning_content
      thinking: message?.reasoning_content || undefined,
      usage: {
        inputTokens: completion.usage?.prompt_tokens || 0,
        outputTokens: completion.usage?.completion_tokens || 0,
        // ç«å±±çš„æ€è€ƒ token åŒ…å«åœ¨ completion_tokens é‡Œï¼Œæ— æ³•å•ç‹¬è·å–
        totalTokens: completion.usage?.total_tokens || 0,
      },
      finishReason: completion.choices?.[0]?.finish_reason,
      raw: completion,
    }
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬ï¼ˆéæµå¼ï¼‰
   */
  async generate(params: TextGenerateParams): Promise<UnifiedTextResult> {
    const model = params.model || this.capabilities.defaultTextModel
    const options = this.translateParams(params, model)

    // æ‰“å°è¯·æ±‚æ—¥å¿—
    logAIRequest({
      title: "ç«å±±å¼•æ“æ–‡æœ¬ç”Ÿæˆ",
      model,
      modelDisplayName: model,
      temperature: options.temperature,
      maxOutputTokens: options.max_tokens,
      systemInstruction: params.systemPrompt,
      prompt: params.userPrompt,
      extraParams: {
        Provider: "ç«å±±å¼•æ“",
        IsThinkingModel: model.includes("seed"),
      },
    })

    const startTime = Date.now()

    const completion = await volcClient.chat.completions.create(options)

    const result = this.translateResponse(completion)
    const durationMs = Date.now() - startTime

    // æ‰“å°æ€ç»´é“¾å†…å®¹
    if (result.thinking) {
      devLog("\nğŸ§  æ€ç»´é“¾å†…å®¹:")
      devLog("-".repeat(40))
      devLog(result.thinking.length > 1000
        ? result.thinking.slice(0, 1000) + `...(å…± ${result.thinking.length} å­—)`
        : result.thinking)
      devLog("-".repeat(40))
    }

    // æ‰“å°å“åº”æ—¥å¿—
    logAIResponse({
      title: "ç«å±±å¼•æ“æ–‡æœ¬ç”Ÿæˆ",
      success: true,
      durationMs,
      finishReason: result.finishReason,
      contentLength: result.content.length,
      usage: {
        promptTokenCount: result.usage.inputTokens,
        candidatesTokenCount: result.usage.outputTokens,
        totalTokenCount: result.usage.totalTokens,
      },
    })

    return result
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬ï¼ˆæµå¼ï¼‰
   */
  async *generateStream(
    params: TextGenerateParams
  ): AsyncGenerator<TextStreamChunk, void, unknown> {
    const model = params.model || this.capabilities.defaultTextModel
    const options = this.translateParams({ ...params, stream: true }, model)

    // æ‰“å°è¯·æ±‚æ—¥å¿—
    logAIRequest({
      title: "ç«å±±å¼•æ“æ–‡æœ¬ç”Ÿæˆ (æµå¼)",
      model,
      modelDisplayName: model,
      temperature: options.temperature,
      maxOutputTokens: options.max_tokens,
      systemInstruction: params.systemPrompt,
      prompt: params.userPrompt,
      extraParams: {
        Provider: "ç«å±±å¼•æ“",
        IsThinkingModel: model.includes("seed"),
      },
    })

    const startTime = Date.now()

    const stream = await volcClient.chat.completions.create(options)

    let usage: TokenUsage | undefined
    let contentLength = 0
    let thinkingLength = 0

    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    for await (const chunk of stream as any) {
      const delta = chunk.choices?.[0]?.delta

      // ç«å±±æµå¼è¿”å› reasoning_contentï¼ˆæ€è€ƒå†…å®¹ï¼‰
      if (delta?.reasoning_content) {
        thinkingLength += delta.reasoning_content.length
        yield { type: "thinking", text: delta.reasoning_content }
      }

      // æ­£å¸¸å†…å®¹
      if (delta?.content) {
        contentLength += delta.content.length
        yield { type: "content", text: delta.content }
      }

      // æœ€åä¸€ä¸ª chunk å¯èƒ½åŒ…å« usage
      if (chunk.usage) {
        usage = {
          inputTokens: chunk.usage.prompt_tokens || 0,
          outputTokens: chunk.usage.completion_tokens || 0,
          totalTokens: chunk.usage.total_tokens || 0,
        }
      }
    }

    // æµç»“æŸåè¿”å› token ä½¿ç”¨ç»Ÿè®¡
    if (usage) {
      yield { type: "usage", usage }
    }

    // æ‰“å°æµå®Œæˆæ—¥å¿—
    logStreamComplete({
      title: "ç«å±±å¼•æ“æ–‡æœ¬ç”Ÿæˆ (æµå¼)",
      durationMs: Date.now() - startTime,
      generatedContentLength: contentLength,
      usage: usage
        ? {
            promptTokenCount: usage.inputTokens,
            candidatesTokenCount: usage.outputTokens,
            totalTokenCount: usage.totalTokens,
          }
        : undefined,
      thinkingContent:
        thinkingLength > 0 ? `(${thinkingLength} å­—æ€è€ƒå†…å®¹)` : undefined,
    })
  }
}
